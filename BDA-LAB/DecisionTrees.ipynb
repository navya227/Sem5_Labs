{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17758a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Setting the environment variables to ensure PySpark uses the correct Python executable\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing a Spark session with a specified amount of driver memory and a name for the app\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_4').getOrCreate()\n",
    "\n",
    "# Reading data from a CSV file without headers and inferring the data schema\n",
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"data/covtype.data\")\n",
    "data_without_header.printSchema()\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Renaming the columns appropriately for the dataset and casting the target variable to DoubleType\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \"Horizontal_Distance_To_Hydrology\", \"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\", \"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \"Horizontal_Distance_To_Fire_Points\"] + [f\"Wilderness_Area_{i}\" for i in range(4)] + [f\"Soil_Type_{i}\" for i in range(40)] + [\"Cover_Type\"]\n",
    "data = data_without_header.toDF(*colnames).withColumn(\"Cover_Type\", col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()\n",
    "\n",
    "# Splitting the data into training and test sets with a 90/10 split\n",
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Caching the datasets to improve performance during training and testing phases\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Assembling a feature vector from all input columns except the target for use in machine learning algorithms\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate=False)\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Initializing and training a decision tree classifier using the assembled feature vectors\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "\n",
    "# Outputting the tree structure of the trained model\n",
    "print(model.toDebugString)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Displaying feature importance as a DataFrame and sorting it in descending order\n",
    "pd.DataFrame(model.featureImportances.toArray(), index=input_cols, columns=['importance']).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "# Applying the model to the training data to generate predictions\n",
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").show(10, truncate=False)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Evaluating model performance using accuracy and F1 score metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\", predictionCol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)\n",
    "\n",
    "# Creating a confusion matrix from the predictions\n",
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\", range(1,8)).count().na.fill(0.0).orderBy(\"Cover_Type\")\n",
    "confusion_matrix.show()\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Function to calculate the proportion of each class in the dataset\n",
    "def class_probabilities(data):\n",
    "    total = data.count()\n",
    "    return data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(col(\"count\").cast(DoubleType())).withColumn(\"count_proportion\", col(\"count\")/total).select(\"count_proportion\").collect()\n",
    "\n",
    "# Calculating class probabilities for training and testing data to use for model evaluation\n",
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)\n",
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n",
    "\n",
    "# Calculating the sum of products of probabilities, which can be used to gauge model effectiveness across data splits\n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities, test_prior_probabilities)])\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Setting up a pipeline with vector assembler and classifier for streamlined processing\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"featureVector\", predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Building a parameter grid for model tuning to optimize decision tree parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.impurity, [\"gini\", \"entropy\"]) \\\n",
    "    .addGrid(classifier.maxDepth, [1, 20]) \\\n",
    "    .addGrid(classifier.maxBins, [40, 300]) \\\n",
    "    .addGrid(classifier.minInfoGain, [0.0, 0.05]) \\\n",
    "    .build()\n",
    "\n",
    "# Setting up the evaluator for model tuning to use accuracy as the metric\n",
    "multiclassEval = MulticlassClassificationEvaluator() \\\n",
    "    .setLabelCol(\"Cover_Type\") \\\n",
    "    .setPredictionCol(\"prediction\") \\\n",
    "    .setMetricName(\"accuracy\")\n",
    "\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "\n",
    "# Configuring train-validation split for model selection\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "                                 estimator=pipeline,\n",
    "                                 evaluator=multiclassEval,\n",
    "                                 estimatorParamMaps=paramGrid,\n",
    "                                 trainRatio=0.9)\n",
    "\n",
    "# Fitting the train-validation split to find the best model parameters\n",
    "validator_model = validator.fit(train_data)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Extracting and printing the best model parameters\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())\n",
    "\n",
    "# Re-fitting the validator to retrieve metrics\n",
    "validator_model = validator.fit(train_data)\n",
    "metrics = validator_model.validationMetrics\n",
    "params = validator_model.getEstimatorParamMaps()\n",
    "metrics_and_params = list(zip(metrics, params))\n",
    "\n",
    "# Sorting the metrics and parameters by performance and printing the best result\n",
    "metrics_and_params.sort(key=lambda x: x[0], reverse=True)\n",
    "metrics_and_params\n",
    "metrics.sort(reverse=True)\n",
    "print(metrics[0])\n",
    "\n",
    "# Evaluating the best model on the test data set\n",
    "multiclassEval.evaluate(best_model.transform(test_data))\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Defining a function to decode one-hot encoded vectors to single integer labels\n",
    "def unencode_one_hot(data):\n",
    "    wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "    wilderness_assembler = VectorAssembler().\\\n",
    "        setInputCols(wilderness_cols).\\\n",
    "        setOutputCol(\"wilderness\")\n",
    "    unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n",
    "    with_wilderness = wilderness_assembler.transform(data).\\\n",
    "        drop(*wilderness_cols).\\\n",
    "        withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n",
    "\n",
    "    soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "    soil_assembler = VectorAssembler().\\\n",
    "        setInputCols(soil_cols).\\\n",
    "        setOutputCol(\"soil\")\n",
    "    with_soil = soil_assembler.\\\n",
    "        transform(with_wilderness).\\\n",
    "        drop(*soil_cols).\\\n",
    "        withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n",
    "\n",
    "    return with_soil\n",
    "\n",
    "# Applying the function to unencode training data\n",
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()\n",
    "unenc_train_data.groupBy('wilderness').count().show()\n",
    "\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Setting up the feature vector assembler and indexer for machine learning\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "\n",
    "# Configuring the classifier to use indexed vectors\n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").setFeaturesCol(\"indexedVector\").setPredictionCol(\"prediction\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "\n",
    "# Note indicating potential performance issues with the random forest classifier\n",
    "0.0.5 Random Forests Takes Too Long To Run\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Setting up the random forest classifier\n",
    "classifier = RandomForestClassifier(seed=1234, labelCol=\"Cover_Type\", featuresCol=\"indexedVector\", predictionCol=\"prediction\")\n",
    "\n",
    "# Reconfiguring the pipeline for random forests\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "    addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "    addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "    addGrid(classifier.maxBins, [40, 300]). \\\n",
    "    addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "    build()\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "    setLabelCol(\"Cover_Type\"). \\\n",
    "    setPredictionCol(\"prediction\"). \\\n",
    "    setMetricName(\"accuracy\")\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "                                  estimator=pipeline,\n",
    "                                  evaluator=multiclassEval,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  trainRatio=0.9)\n",
    "validator_model = validator.fit(unenc_train_data)\n",
    "\n",
    "# Extracting the best model from the train-validation split\n",
    "best_model = validator_model.bestModel\n",
    "forest_model = best_model.stages[2]\n",
    "\n",
    "# Listing and sorting feature importances from the random forest model\n",
    "feature_importance_list = list(zip(input_cols, forest_model.featureImportances.toArray()))\n",
    "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pprint(feature_importance_list)\n",
    "\n",
    "# Transforming the test data using the best model and showing predictions\n",
    "unenc_test_data = unencode_one_hot(test_data)\n",
    "best_model.transform(unenc_test_data.drop(\"Cover_Type\")).select(\"prediction\").show(1)\n",
    "\n",
    "# Additional setup for the pipeline to handle random forests\n",
    "assembler = VectorAssembler().setInputCols(input_cols).setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().setMaxCategories(40).setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n",
    "\n",
    "# Building a parameter grid to optimize the RandomForestClassifier parameters\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "    addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "    addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "    addGrid(classifier.maxBins, [40, 300]). \\\n",
    "    addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "    build()\n",
    "\n",
    "# Setting up the multiclass evaluator for accuracy metric\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "    setLabelCol(\"Cover_Type\"). \\\n",
    "    setPredictionCol(\"prediction\"). \\\n",
    "    setMetricName(\"accuracy\")\n",
    "\n",
    "# Configuring a TrainValidationSplit for parameter tuning\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "                                  estimator=pipeline,\n",
    "                                  evaluator=multiclassEval,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  trainRatio=0.9)\n",
    "\n",
    "# Fitting the validator to the unencoded training data to find the best RandomForest model\n",
    "validator_model = validator.fit(unenc_train_data)\n",
    "\n",
    "# Extracting the best RandomForest model from the validation process\n",
    "best_model = validator_model.bestModel\n",
    "forest_model = best_model.stages[2]\n",
    "\n",
    "# Printing the sorted feature importance list from the RandomForest model\n",
    "feature_importance_list = list(zip(input_cols, forest_model.featureImportances.toArray()))\n",
    "feature_importance_list.sort(key=lambda x: x[1], reverse=True)\n",
    "pprint(feature_importance_list)\n",
    "\n",
    "# Transforming the test data using the best model and showing the first prediction result\n",
    "unenc_test_data = unencode_one_hot(test_data)\n",
    "best_model.transform(unenc_test_data.drop(\"Cover_Type\")).select(\"prediction\").show(1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
