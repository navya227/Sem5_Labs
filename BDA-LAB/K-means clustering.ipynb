{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2c0969-befd-4f46-b902-c5ec5de0d82a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|cluster|  label|anomaly|\n",
      "+-------+-------+-------+\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "|      1|normal.|      0|\n",
      "+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total entropy of the model: -3.0408280571150432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, Imputer, StringIndexer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from random import randint\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('clustering_anomalies').getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "data_without_header = spark.read.option(\"inferSchema\", True).option(\"header\", False).csv(\"kddcup.data_10_percent_corrected\")\n",
    "column_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \n",
    "                \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \n",
    "                \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \n",
    "                \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \n",
    "                \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \n",
    "                \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \n",
    "                \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\"]\n",
    "data = data_without_header.toDF(*column_names)\n",
    "\n",
    "# Step 1: Handle Missing Values and Scale Numerical Features\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = [\"duration\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \n",
    "                     \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \n",
    "                     \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \n",
    "                     \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \n",
    "                     \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\", \n",
    "                     \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \n",
    "                     \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \n",
    "                     \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\"]\n",
    "\n",
    "categorical_columns = [\"protocol_type\", \"service\", \"flag\"]\n",
    "\n",
    "# Handle missing values for numerical columns\n",
    "imputer = Imputer(strategy=\"mean\").setInputCols(numerical_columns).setOutputCols(numerical_columns)\n",
    "data_imputed = imputer.fit(data).transform(data)\n",
    "\n",
    "# Index categorical columns\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\") for col in categorical_columns]\n",
    "\n",
    "# Scale the numerical features\n",
    "assembler = VectorAssembler(inputCols=numerical_columns + [col + \"_indexed\" for col in categorical_columns], outputCol=\"featureVector\")\n",
    "scaler = StandardScaler(inputCol=\"featureVector\", outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "\n",
    "# Step 2: K-Means Clustering\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans().setK(5).setSeed(randint(100, 100000)).setFeaturesCol(\"scaledFeatureVector\").setPredictionCol(\"cluster\")\n",
    "pipeline = Pipeline(stages=indexers + [assembler, scaler, kmeans])\n",
    "pipeline_model = pipeline.fit(data_imputed)\n",
    "\n",
    "# Get the clustering results\n",
    "data_with_clusters = pipeline_model.transform(data_imputed)\n",
    "\n",
    "# Step 3: Label Data Points as Anomalies Based on Cluster Assignments\n",
    "\n",
    "# Mark the smallest cluster as anomalies (or you can use a threshold for cluster sizes)\n",
    "cluster_sizes = data_with_clusters.groupBy(\"cluster\").count().orderBy(\"count\").collect()\n",
    "smallest_cluster = cluster_sizes[0]['cluster']  # Get the smallest cluster\n",
    "data_with_anomalies = data_with_clusters.withColumn(\"anomaly\", when(col(\"cluster\") == smallest_cluster, 1).otherwise(0))\n",
    "\n",
    "# Step 4: Evaluate the K-Means Clustering Model in Detecting Anomalies\n",
    "\n",
    "# Calculate entropy-based evaluation for anomaly detection\n",
    "def entropy(counts):\n",
    "    total = sum(counts)\n",
    "    probs = [count / total for count in counts]\n",
    "    return -sum([p * F.log2(p) for p in probs if p > 0])\n",
    "\n",
    "cluster_label = data_with_anomalies.select(\"cluster\", \"label\").groupBy(\"cluster\", \"label\").count()\n",
    "window_spec = Window.partitionBy(\"cluster\")\n",
    "cluster_label_with_probs = cluster_label.withColumn(\"cluster_total\", F.sum(\"count\").over(window_spec))\n",
    "cluster_label_with_probs = cluster_label_with_probs.withColumn(\"p\", F.col(\"count\") / F.col(\"cluster_total\"))\n",
    "\n",
    "# Calculate entropy for each cluster\n",
    "cluster_entropy = cluster_label_with_probs.groupBy(\"cluster\").agg(\n",
    "    F.sum(F.col(\"p\") * F.log2(F.col(\"p\"))).alias(\"entropy\")\n",
    ")\n",
    "total_entropy = cluster_entropy.agg(F.sum(\"entropy\")).collect()[0][0]\n",
    "\n",
    "# Show the results\n",
    "data_with_anomalies.select(\"cluster\", \"label\", \"anomaly\").show()\n",
    "print(f\"Total entropy of the model: {total_entropy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b5a0f-3ab2-4511-b82a-585817fecb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
