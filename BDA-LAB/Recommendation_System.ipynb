{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babefbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import os\n",
    "import sys\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# Set up Python environment for PySpark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Initialize SparkContext, handling if it already exists\n",
    "try:\n",
    "    sc = ps.SparkContext('local[*]')  # Create SparkContext on all available CPUs\n",
    "    # sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n",
    "\n",
    "import unittest\n",
    "import sys\n",
    "\n",
    "# Define a unit test for the RDD\n",
    "class TestRdd(unittest.TestCase):\n",
    "    def test_take(self):\n",
    "        input = sc.parallelize([1, 2, 3, 4])\n",
    "        self.assertEqual([1, 2, 3, 4], input.take(4))\n",
    "\n",
    "# Function to run the unit tests\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestRdd)\n",
    "    unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)\n",
    "\n",
    "run_tests()\n",
    "help(sc)\n",
    "\n",
    "import json\n",
    "\n",
    "# Define field sets for various uses\n",
    "fields = ['product_id', 'user_id', 'score', 'time']\n",
    "fields2 = ['product_id', 'user_id', 'review', 'profile_name', 'helpfulness', 'score', 'time']\n",
    "fields3 = ['product_id', 'user_id', 'time']\n",
    "fields4 = ['user_id', 'score', 'time']\n",
    "\n",
    "# Validate the JSON line to ensure it has necessary fields\n",
    "def validate(line):\n",
    "    for field in fields2:\n",
    "        if field not in line:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Load and process reviews data\n",
    "reviews_raw = sc.textFile('data/movies.json')\n",
    "reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n",
    "reviews.cache()\n",
    "\n",
    "# Display sample data\n",
    "reviews.take(1)\n",
    "\n",
    "# Calculate statistics for movies, users, and entries\n",
    "num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n",
    "num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n",
    "num_entries = reviews.count()\n",
    "print(f\"{num_entries} reviews of {num_movies} movies by {num_users} different people.\")\n",
    "\n",
    "# Calculate most-watched movies\n",
    "r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n",
    "avg3 = r1.mapValues(lambda x: (x, 1)) \\\n",
    "          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "          .filter(lambda x: x[1][1] > 20) \\\n",
    "          .map(lambda x: ((x[1][0] + x[1][1],), x[0])) \\\n",
    "          .sortByKey(ascending=False)\n",
    "\n",
    "# Display top 10 most-watched movies\n",
    "for movie in avg3.take(10):\n",
    "    print(f\"http://www.amazon.com/dp/{movie[1][0]} WATCHED BY: {movie[0][0]} PEOPLE\")\n",
    "\n",
    "# Calculate users with most reviews\n",
    "r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n",
    "avg2 = r2.mapValues(lambda x: (x, 1)) \\\n",
    "          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "          .filter(lambda x: x[1][1] > 20) \\\n",
    "          .map(lambda x: ((x[1][0] + x[1][1],), x[0])) \\\n",
    "          .sortByKey(ascending=False)\n",
    "\n",
    "# Display top 10 users by review count\n",
    "for movie in avg2.take(10):\n",
    "    print(f\"http://www.amazon.com/dp/{movie[1][0]} WATCHED: {movie[0][0]} MOVIES\")\n",
    "\n",
    "# Find reviews by specific profile name\n",
    "filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n",
    "print(f\"Found {filtered.count()} entries.\\n\")\n",
    "for review in filtered.collect():\n",
    "    print(f\"Rating: {review['score']} and helpfulness: {review['helpfulness']}\")\n",
    "    print(f\"http://www.amazon.com/dp/{review['product_id']}\")\n",
    "    print(review['summary'])\n",
    "    print(review['review'])\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Calculate best and worst rated movies\n",
    "reviews_by_movie = reviews.map(lambda r: ((r['product_id'],), r['score']))\n",
    "avg = reviews_by_movie.mapValues(lambda x: (x, 1)) \\\n",
    "                      .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "                      .filter(lambda x: x[1][1] > 20) \\\n",
    "                      .map(lambda x: ((x[1][0] / x[1][1],), x[0])) \\\n",
    "                      .sortByKey(ascending=True)\n",
    "\n",
    "# Display top 10 best and worst rated movies\n",
    "for movie in avg.take(10):\n",
    "    print(f\"http://www.amazon.com/dp/{movie[1][0]} Rating: {movie[0][0]}\")\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Process review time series data\n",
    "timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'], 'time': datetime.fromtimestamp(entry['time'])})\n",
    "sample = timeseries_rdd.sample(withReplacement=False, fraction=20000.0 / num_entries, seed=1134)\n",
    "timeseries = pd.DataFrame(sample.collect(), columns=['score', 'time'])\n",
    "\n",
    "# Resample and plot time series data\n",
    "timeseries.set_index('time', inplace=True)\n",
    "Rsample = timeseries.score.resample('Y').count()\n",
    "Rsample.plot()\n",
    "Rsample2 = timeseries.score.resample('M').count()\n",
    "Rsample2.plot()\n",
    "Rsample3 = timeseries.score.resample('Q').count()\n",
    "Rsample3.plot()\n",
    "\n",
    "# Plot average rating of movies\n",
    "for movie in avg.take(4):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \"AVERAGE RATING OF MOVIE\"')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('AVGRATING')\n",
    "\n",
    "# Plot number of movies reviewed by users\n",
    "for movie in avg2.take(3):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \"NUMBER OF MOVIES REVIEWED BY USER\"')\n",
    "    plt.xlabel('USER')\n",
    "    plt.ylabel('MOVIE COUNT')\n",
    "\n",
    "# Plot movies reviewed by number of users\n",
    "for movie in avg3.take(4):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \"MOVIES REVIEWED BY NUMBER OF USERS\"')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('USER COUNT')\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "from numpy import array\n",
    "import hashlib\n",
    "\n",
    "# Helper function to hash user and product IDs\n",
    "def get_hash(s):\n",
    "    return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "# Prepare ratings data for recommendation model\n",
    "ratings = reviews.map(lambda entry: tuple([get_hash(entry['user_id'].encode('utf-8')),\n",
    "                                           get_hash(entry['product_id'].encode('utf-8')),\n",
    "                                           int(entry['score'])]))\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data = ratings.filter(lambda entry: ((entry[0] + entry[1]) % 10) >= 2)\n",
    "test_data = ratings.filter(lambda entry: ((entry[0] + entry[1]) % 10) < 2)\n",
    "train_data.cache()\n",
    "print(f\"Number of train samples: {train_data.count()}\")\n",
    "print(f\"Number of test samples: {test_data.count()}\")\n",
    "\n",
    "# Train recommendation model using ALS\n",
    "rank = 20\n",
    "numIterations = 20\n",
    "model = ALS.train(train_data, rank, numIterations)\n",
    "\n",
    "# Evaluate model on test data\n",
    "unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))\n",
    "predictions = model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))\n",
    "true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)\n",
    "MSE = true_and_predictions.map(lambda r: (int(r[1][0]) - int(r[1][1]) ** 2)).reduce(lambda x, y: x + y) / true_and_predictions.count()\n",
    "true_and_predictions.take(10)\n",
    "\n",
    "# Sentiment analysis on reviews\n",
    "min_occurrences = 10\n",
    "good_reviews = reviews.filter(lambda line: line['score'] == 5.0)\n",
    "bad_reviews = reviews.filter(lambda line: line['score'] == 1.0)\n",
    "\n",
    "# Process and count word frequencies in good and bad reviews\n",
    "good_words = good_reviews.flatMap(lambda line: line['review'].split(' ')).map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a + b).filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' ')).map(lambda word: (word.strip(), 1)).reduceByKey(lambda a, b: a + b).filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "\n",
    "# Calculate word frequencies\n",
    "num_good_words = good_words.count()\n",
    "num_bad_words = bad_words.count()\n",
    "frequency_good = good_words.map(lambda word: ((word[0],), float(word[1]) / num_good_words))\n",
    "frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1]) / num_bad_words))\n",
    "\n",
    "# Join frequencies and calculate relative differences\n",
    "joined_frequencies = frequency_good.join(frequency_bad)\n",
    "result = joined_frequencies.map(lambda f: ((abs(f[1][0] - f[1][1]) / f[1][0],), f[0][0])).sortByKey(ascending=False)\n",
    "result.take(50)\n",
    "\n",
    "# Plot sentiment analysis histogram\n",
    "for movie in result.take(7):\n",
    "    plt.bar(movie[1], movie[0][0])\n",
    "    plt.title('Histogram of \"SENTIMENT ANALYSIS\"')\n",
    "    plt.xlabel('WORD')\n",
    "    plt.ylabel('NUMBER OF OCCURRENCES')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
