{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787fad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Setting environment variables to specify Python executables for Spark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing a Spark session with specified memory and an application name\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()\n",
    "\n",
    "# Loading data from a CSV file into a DataFrame without initially specifying schema\n",
    "prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "prev\n",
    "\n",
    "# Displaying the inferred schema of the data as strings\n",
    "prev.show(2)\n",
    "\n",
    "# Reading the same data with options set for headers, schema inference, and handling of null values\n",
    "parsed = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"nullValue\", \"?\").csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "\n",
    "# Printing the schema to see data types and structure\n",
    "parsed.printSchema()\n",
    "\n",
    "# Displaying the first five rows of the dataset\n",
    "parsed.show(5)\n",
    "\n",
    "# Counting the total number of rows in the DataFrame\n",
    "parsed.count()\n",
    "\n",
    "# Caching the DataFrame in memory for faster access\n",
    "parsed.cache()\n",
    "\n",
    "# Displaying the schema of the DataFrame now with data types and caching confirmation\n",
    "parsed.printSchema()\n",
    "\n",
    "# Grouping data by the 'is_match' field and counting occurrences, then ordering by count descending\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# Registering the DataFrame as a temporary SQL table for queries\n",
    "parsed.createOrReplaceTempView(\"linkage\")\n",
    "\n",
    "# Executing a SQL query to count matches and non-matches from the temp view\n",
    "spark.sql(\"\"\"\n",
    "SELECT is_match, COUNT(*) cnt\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "ORDER BY cnt DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Generating summary statistics for all columns in the DataFrame\n",
    "summary = parsed.describe()\n",
    "\n",
    "# Selecting specific statistical results for certain columns\n",
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()\n",
    "\n",
    "# Filtering rows where matches are true and describing the statistics for these rows\n",
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "\n",
    "# Filtering rows where matches are false and describing the statistics for these rows\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()\n",
    "\n",
    "# Converting the summary DataFrame to a Pandas DataFrame\n",
    "summary_p = summary.toPandas()\n",
    "\n",
    "# Displaying the head of the Pandas DataFrame\n",
    "summary_p.head()\n",
    "\n",
    "# Checking the shape of the Pandas DataFrame\n",
    "summary_p.shape\n",
    "\n",
    "# Setting the index of the Pandas DataFrame and transposing it\n",
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "\n",
    "# Renaming columns after transposition\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "\n",
    "# Removing the index name axis\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "\n",
    "# Checking the shape again after transformations\n",
    "summary_p.shape\n",
    "\n",
    "# Converting the Pandas DataFrame back to a Spark DataFrame\n",
    "summaryT = spark.createDataFrame(summary_p)\n",
    "\n",
    "# Printing the schema of the new Spark DataFrame\n",
    "summaryT.printSchema()\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Casting all columns except 'field' to DoubleType to ensure numerical operations can be performed\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "\n",
    "# Printing the schema again to confirm type changes\n",
    "summaryT.printSchema()\n",
    "\n",
    "# Defining a function to automate the summary statistic transposition and conversion\n",
    "def pivot_summary(desc):\n",
    "    # Convert to pandas dataframe\n",
    "    desc_p = desc.toPandas()\n",
    "    # Transpose\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    # Convert to Spark dataframe\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    # Convert metric columns to double from string\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n",
    "\n",
    "# Applying the function to match and miss summaries\n",
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)\n",
    "\n",
    "# Creating temporary views for SQL operations\n",
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "\n",
    "# Executing a SQL query to compute differences between match and miss summaries\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "ORDER BY delta DESC, total DESC\n",
    "\"\"\")\n",
    "\n",
    "# Defining a list of good features based on domain knowledge\n",
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "\n",
    "# Creating an expression string to sum the good features for scoring\n",
    "sum_expression = \" + \".join(good_features)\n",
    "\n",
    "# Filling NA values in good features, computing a score, and selecting relevant columns\n",
    "scored = parsed.fillna(0, subset=good_features).withColumn('score', expr(sum_expression)).select('score', 'is_match')\n",
    "\n",
    "# Defining a function to create cross tabs for score thresholds\n",
    "def crossTabs(scored: DataFrame, t: DoubleType) -> DataFrame:\n",
    "    return scored.selectExpr(f\"score >= {t} as above\", \"is_match\").groupBy(\"above\").pivot(\"is_match\", [\"true\", \"false\"]).count()\n",
    "\n",
    "# Displaying crosstab results for different score thresholds\n",
    "crossTabs(scored, 4.0).show()\n",
    "crossTabs(scored, 2.0).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85445658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.sql.types import ArrayType, StringType, DoubleType\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Set up the Spark session\n",
    "spark = SparkSession.builder.appName(\"EntityResolution\").getOrCreate()\n",
    "\n",
    "# Load dataset\n",
    "data = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "\n",
    "# Define a UDF for normalization (e.g., lowercasing, removing non-alphanumeric characters)\n",
    "def normalize(text):\n",
    "    if text is None:\n",
    "        return []\n",
    "    return ''.join(filter(str.isalnum, text.lower())).split()\n",
    "\n",
    "normalize_udf = udf(normalize, ArrayType(StringType()))\n",
    "\n",
    "# Apply the tokenizer and normalization to the relevant columns\n",
    "tokenizer = Tokenizer(inputCol=\"cmp_fname_c1\", outputCol=\"tokenized\")\n",
    "data = tokenizer.transform(data)\n",
    "data = data.withColumn(\"normalized\", normalize_udf(col(\"tokenized\")))\n",
    "\n",
    "# Define a UDF to compute Jaccard similarity between two arrays of tokens\n",
    "def jaccard_similarity(list1, list2):\n",
    "    if not list1 or not list2:\n",
    "        return 0.0\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    return float(len(set1 & set2) / len(set1 | set2))\n",
    "\n",
    "jaccard_udf = udf(jaccard_similarity, DoubleType())\n",
    "\n",
    "# Compute the similarity score for the records, assuming 'normalized' column for both records\n",
    "data = data.alias(\"df1\").join(data.alias(\"df2\"), \"id_1\")  # Self-join to compare records\n",
    "data = data.withColumn(\"similarity_score\", jaccard_udf(col(\"df1.normalized\"), col(\"df2.normalized\")))\n",
    "\n",
    "# Add prediction column based on similarity score threshold\n",
    "threshold = 0.5  # Threshold for determining whether the pair is a match\n",
    "data = data.withColumn(\"prediction\", (col(\"similarity_score\") > threshold).cast(\"integer\"))\n",
    "\n",
    "# Select relevant columns to evaluate the model\n",
    "data = data.select(col(\"is_match\").cast(\"integer\").alias(\"label\"), \"prediction\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "precision = evaluator.evaluate(data, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(data, {evaluator.metricName: \"weightedRecall\"})\n",
    "f1_score = evaluator.evaluate(data, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Output the evaluation metrics\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
